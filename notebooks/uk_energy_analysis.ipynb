{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f55927",
   "metadata": {},
   "source": [
    "# UK Electricity Demand & Generation Forecasting: Technical Assignment\n",
    "\n",
    "This notebook demonstrates a workflow for forecasting UK electricity demand using real-world data and machine learning. \n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Structure\n",
    "\n",
    "### 1. **Business Context & Problem Statement**\n",
    "- **Business**: Energy trading company with narrow profit margins, relying on accurate short-term demand forecasts.\n",
    "- **Problem**: The current naive baseline forecast shows significant prediction errors, impacting trading performance.\n",
    "- **Goal**: Develop and evaluate advanced forecasting models, and test additional features to improve 0–48 hour demand prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Data Acquisition & Preparation**\n",
    "- **Datasets**:\n",
    "    - UK electricity demand (2009–2024)\n",
    "    - UK electricity generation by source (2009–2024)\n",
    "- **Data Loading**:\n",
    "    - Download demand data from Kaggle and generation data from NESO API.\n",
    "    - Parse, clean, and join both datasets on a common `datetime` column.\n",
    "- **Preprocessing**:\n",
    "    - Handle time intervals (settlement periods), missing values, and daylight saving issues.\n",
    "    - Drop unnecessary columns and ensure consistent datetime formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Exploratory Data Analysis (EDA)**\n",
    "- **Visualization**:\n",
    "    - Plot hourly, daily, weekly, monthly, and yearly averages for demand and generation.\n",
    "    - Use interactive Plotly charts to explore trends and seasonality.\n",
    "- **Insights**:\n",
    "    - Identify patterns, anomalies, and potential feature engineering opportunities.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Baseline Model Definition**\n",
    "- **Model**: 7-day mean demand at the same time of day (simple historical average).\n",
    "- **Evaluation**:\n",
    "    - Predict demand in rolling 48-hour windows for December 2023.\n",
    "    - Visualize predictions vs. actuals.\n",
    "    - Calculate RMSE and MAPE for each 48-hour window.\n",
    "    - Calculate mean RMSE and MAPE for all windows that month.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Machine Learning Models**\n",
    "- **Model 1**: Gradient Boosting Regressor (GBR) with 3 features:\n",
    "    - Day of week, hour of day, 24 h lagged demand.\n",
    "- **Model 2**: GBR with 5 features (adds):\n",
    "    - Day of year, 30 min lagged demand.\n",
    "- **Training & Testing**:\n",
    "    - Train on Janurary 2020 – November 2023 data, test on December 2023.\n",
    "    - Make predictions for rolling 48-hour windows.\n",
    "    - Calculate RMSE and MAPE for each 48-hour window.\n",
    "    - Calculate mean RMSE and MAPE for all windows that month.\n",
    "- **Visualization**:\n",
    "    - Plot model predictions vs. actuals for each window.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Model Comparison**\n",
    "- **Metrics**:\n",
    "    - Compare RMSE across baseline and both ML models for each day in December 2023.\n",
    "- **Visualization**:\n",
    "    - Plot MAPE for all models to highlight improvements from feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Feature Engineering & Further Improvements**\n",
    "- **Analysis**:\n",
    "    - Discuss impact of additional features (lags, seasonality).\n",
    "    - Identify which features most improve forecast accuracy.\n",
    "- **Next Steps**:\n",
    "    - Suggest further data sources (weather, market prices, holidays).\n",
    "    - Propose advanced modeling approaches (e.g., deep learning, exogenous variables).\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This notebook provides a reproducible, step-by-step approach to:\n",
    "- Data ingestion and cleaning\n",
    "- Exploratory analysis\n",
    "- Baseline and advanced ML modeling\n",
    "- Model evaluation and comparison\n",
    "- Feature engineering insights\n",
    "\n",
    "It is designed to showcase both technical skills and business understanding for a real-world energy forecasting challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679be8fa",
   "metadata": {},
   "source": [
    "### Imports and Library Setup\n",
    "\n",
    "Import all the necessary Python libraries and packages for the notebook's workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9316437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadimbogatyr/VSCode/adaptfy_uk_energy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262de04",
   "metadata": {},
   "source": [
    "### UK Electricity Demand Data Acquisition and Preparation\n",
    "\n",
    "Downloading the data from Kaggle (https://www.kaggle.com/datasets/albertovidalrod/electricity-consumption-uk-20092022/data). \n",
    "\n",
    "The time is stored in 'settlement_period' as 1-48 30-minute windows. I convert them to hours and minutes, merge with date to make datetime format column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a528e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Download all UK electricity demand files from Kaggle dataset\n",
    "path = kagglehub.dataset_download(\"albertovidalrod/electricity-consumption-uk-20092022\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Copy historic_demand_2009_2024.csv to data folder in the parent directory (where notebooks folder is)\n",
    "csv_name = \"historic_demand_2009_2024.csv\"\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(parent_dir, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "src = os.path.join(path, csv_name)\n",
    "dst = os.path.join(data_dir, csv_name)\n",
    "shutil.copy(src, dst)\n",
    "print(f\"Copied {csv_name} to data folder: {dst}\")\n",
    "\n",
    "# Read CSV directly with Polars\n",
    "df_dmnd = pl.read_csv(dst)\n",
    "\n",
    "# Calculate datetime for each settlement_period (1-48, each is a 30-min interval)\n",
    "df_dmnd = df_dmnd.with_columns(\n",
    "    (pl.col('settlement_date') + \" \" + pl.format(\"{}:{}\", \n",
    "        ((pl.col('settlement_period') - 1) * 30 // 60).cast(pl.Utf8).str.zfill(2),\n",
    "        ((pl.col('settlement_period') - 1) * 30 % 60).cast(pl.Utf8).str.zfill(2)\n",
    "    )).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M\", strict=False).alias('datetime')\n",
    ")\n",
    "\n",
    "# Drop rows where datetime is null (due to day-time saving time issues)\n",
    "df_dmnd = df_dmnd.filter(pl.col('datetime').is_not_null())\n",
    "\n",
    "# Sort by datetime (Polars does not have a true index)\n",
    "df_dmnd = df_dmnd.sort('datetime')\n",
    "\n",
    "# Drop unwanted columns\n",
    "cols_to_drop = [\"settlement_period\", \"settlement_date\", \"\"] \n",
    "df_dmnd = df_dmnd.drop(cols_to_drop)\n",
    "\n",
    "# Reorder columns to have datetime first\n",
    "cols = ['datetime'] + [col for col in df_dmnd.columns if col != 'datetime']\n",
    "df_dmnd = df_dmnd.select(cols)\n",
    "\n",
    "print(\"Demand DataFrame columns:\", df_dmnd.columns)\n",
    "print(df_dmnd.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabddbf",
   "metadata": {},
   "source": [
    "### UK Electricity Generation Data Acquisition and Preparation\n",
    "\n",
    "NESO Energy data: https://www.neso.energy/data-portal/historic-generation-mix/historic_gb_generation_mix#\n",
    "\n",
    "Retrieve electricity generation data using API. Parse and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0480d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# NESO energy generation mix resource_id from the example\n",
    "resource_id = \"f93d1835-75bc-43e5-84ad-12472b180a98\"\n",
    "\n",
    "# CKAN API endpoint\n",
    "url = \"https://api.neso.energy/api/3/action/datastore_search\"\n",
    "\n",
    "# Query parameters\n",
    "params = {\n",
    "    \"resource_id\": resource_id,\n",
    "    \"limit\": 500000  # Adjust limit as needed\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Convert records to Polars DataFrame\n",
    "records = data['result']['records']\n",
    "df_gnrt = pl.DataFrame(records)\n",
    "\n",
    "# Parse DATETIME to polars datetime\n",
    "df_gnrt = df_gnrt.with_columns(\n",
    "    pl.col(\"DATETIME\").str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S\", strict=False).alias(\"datetime\")\n",
    ")\n",
    "\n",
    "# Split into date and time columns\n",
    "df_gnrt = df_gnrt.with_columns([\n",
    "    pl.col(\"datetime\").dt.strftime(\"%Y-%m-%d\").alias(\"date\"),\n",
    "    pl.col(\"datetime\").dt.strftime(\"%H:%M\").alias(\"time\")\n",
    "])\n",
    "\n",
    "# Drop 'DATETIME', '_id', and any Unnamed columns\n",
    "cols_to_drop = [\"DATETIME\", \"_id\", \"date\", \"time\"]\n",
    "df_gnrt = df_gnrt.drop(cols_to_drop)\n",
    "\n",
    "# Lowercase all column names\n",
    "df_gnrt.columns = [col.lower() for col in df_gnrt.columns]\n",
    "\n",
    "# Bring 'datetime' to the beginning\n",
    "df_gnrt = df_gnrt.select([\"datetime\"] + [col for col in df_gnrt.columns if col != \"datetime\"])\n",
    "\n",
    "print(\"Demand DataFrame columns:\", df_gnrt.columns)\n",
    "print(df_gnrt.head(10))\n",
    "\n",
    "print(\"Dataframe shape:\", df_gnrt.shape)\n",
    "\n",
    "# Save to CSV in the data folder (in the same folder as the notebooks folder)\n",
    "gen_csv_name = \"historic_generation_2009_2024.csv\"\n",
    "gen_csv_path = os.path.join(parent_dir, \"data\", gen_csv_name)\n",
    "df_gnrt.write_csv(gen_csv_path)\n",
    "print(f\"Saved generation data to: {gen_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d5287",
   "metadata": {},
   "source": [
    "Merge demand and generation data on the `datetime` column to create a unified DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cbc99",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Merge the two dataframes on 'datetime'\n",
    "df = df_dmnd.join(df_gnrt, on=[\"datetime\"], how=\"inner\")\n",
    "df = df.sort(\"datetime\")\n",
    "\n",
    "# Print the column names and the header\n",
    "print(\"Columns in the new dataframe: \", df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09694589",
   "metadata": {},
   "source": [
    "### Plot Demand and Generation data. \n",
    "\n",
    "Reusable function to plot interactive time series of National Demand, Transmission System Demand, and Generation using Plotly. Allows flexible time ranges, labels, and styling for clear comparison of trends.\n",
    "\n",
    "From the dataset description: https://www.kaggle.com/datasets/albertovidalrod/electricity-consumption-uk-20092022/data\n",
    "\n",
    "**ND (National Demand)**. National Demand is the sum of metered generation, but excludes generation required to meet station load, pump storage pumping and interconnector exports. National Demand is calculated as a sum of generation based on National Grid ESO operational generation metering. Measured in MW.\n",
    "\n",
    "**TSD (Transmission System Demand)**. Transmission System Demand is equal to the ND plus the additional generation required to meet station load, pump storage pumping and interconnector exports. Measured in MW.\n",
    "\n",
    "We are going to refer to TSD as demand from here on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d32d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_avg_plotly(\n",
    "    df_avg,\n",
    "    x_col,\n",
    "    start,\n",
    "    end,\n",
    "    title,\n",
    "    xlabel,\n",
    "    marker_size,\n",
    "    line_style,\n",
    "    nd_color='bisque',\n",
    "    tsd_color='darkorange',\n",
    "    gen_color='dodgerblue'\n",
    "):\n",
    "    \n",
    "    # Filter the dataframe based on the provided start and end datetime\n",
    "    df_avg = df_avg.filter((pl.col(x_col) >= start) & (pl.col(x_col) <= end))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_avg[x_col],\n",
    "        y=df_avg['nd'],\n",
    "        mode=line_style,\n",
    "        name='National Demand (ND)',\n",
    "        marker=dict(size=marker_size, color=nd_color),\n",
    "        line=dict(color=nd_color)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_avg[x_col],\n",
    "        y=df_avg['tsd'],\n",
    "        mode=line_style,\n",
    "        name='Transmission System Demand (TSD)',\n",
    "        marker=dict(size=marker_size, color=tsd_color),\n",
    "        line=dict(color=tsd_color)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_avg[x_col],\n",
    "        y=df_avg['generation'],\n",
    "        mode=line_style,\n",
    "        name='Generation',\n",
    "        marker=dict(size=marker_size, color=gen_color),\n",
    "        line=dict(color=gen_color)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=20), x=0.5),\n",
    "        xaxis_title=xlabel,\n",
    "        yaxis_title='MW',\n",
    "        yaxis=dict(range=[0, 60000], title_font=dict(size=18), tickfont=dict(size=16)),\n",
    "        xaxis=dict(title_font=dict(size=18), tickfont=dict(size=16)),\n",
    "        legend=dict(font=dict(size=15), x=0, y=0, xanchor='left', yanchor='bottom'),\n",
    "        width=1100,\n",
    "        height=500,\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363ab26",
   "metadata": {},
   "source": [
    "Aggregate demand and generation data to hourly, daily, weekly, monthly, and yearly averages using Polars. \n",
    "\n",
    "Plot each timescale with interactive Plotly charts for trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c84f09",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate averages for different time periods using Polars queries\n",
    "hourly_avg = (\n",
    "    df.group_by_dynamic(\"datetime\", every=\"1h\")\n",
    "      .agg([pl.col(\"nd\").mean(), pl.col(\"tsd\").mean(), pl.col(\"generation\").mean()])\n",
    "      .sort(\"datetime\")\n",
    ")\n",
    "daily_avg = (\n",
    "    df.group_by_dynamic(\"datetime\", every=\"1d\")\n",
    "      .agg([pl.col(\"nd\").mean(), pl.col(\"tsd\").mean(), pl.col(\"generation\").mean()])\n",
    "      .sort(\"datetime\")\n",
    ")\n",
    "weekly_avg = (\n",
    "    df.group_by_dynamic(\"datetime\", every=\"1w\")\n",
    "      .agg([pl.col(\"nd\").mean(), pl.col(\"tsd\").mean(), pl.col(\"generation\").mean()])\n",
    "      .sort(\"datetime\")\n",
    ")\n",
    "monthly_avg = (\n",
    "    df.group_by_dynamic(\"datetime\", every=\"1mo\")\n",
    "      .agg([pl.col(\"nd\").mean(), pl.col(\"tsd\").mean(), pl.col(\"generation\").mean()])\n",
    "      .sort(\"datetime\")\n",
    ")\n",
    "yearly_avg = (\n",
    "    df.group_by_dynamic(\"datetime\", every=\"1y\")\n",
    "      .agg([pl.col(\"nd\").mean(), pl.col(\"tsd\").mean(), pl.col(\"generation\").mean()])\n",
    "      .sort(\"datetime\")\n",
    ")\n",
    "\n",
    "# Define start and end datetimes for each aggregation plot\n",
    "hourly_start = datetime(2023, 7, 1, 0, 0)\n",
    "hourly_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "daily_start = datetime(2023, 1, 1, 0, 0)\n",
    "daily_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "weekly_start = datetime(2018, 1, 1, 0, 0)\n",
    "weekly_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "monthly_start = datetime(2009, 1, 3, 0, 0)\n",
    "monthly_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "yearly_start = datetime(2009, 1, 3, 0, 0)\n",
    "yearly_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "# Usage for each aggregation:\n",
    "plot_avg_plotly(yearly_avg, 'datetime', yearly_start, yearly_end, 'Yearly Average Electricity Demand and Generation in the UK from 2009 to 2024', 'Year', marker_size=6, line_style='lines+markers')\n",
    "plot_avg_plotly(monthly_avg, 'datetime', monthly_start, monthly_end, 'Monthly Average Electricity Demand and Generation in the UK from 2009 to 2024', 'Month', marker_size=4, line_style='lines+markers')\n",
    "plot_avg_plotly(weekly_avg, 'datetime', weekly_start, weekly_end, 'Weekly Average Electricity Demand and Generation in the UK from 2018 to 2023', 'Week', marker_size=3, line_style='lines+markers')\n",
    "plot_avg_plotly(daily_avg, 'datetime', daily_start, daily_end, 'Daily Average Electricity Demand and Generation in the UK during 2023', 'Date', marker_size=4, line_style='lines+markers')\n",
    "plot_avg_plotly(df, 'datetime', hourly_start, hourly_end, 'Hourly Average Electricity Demand and Generation in the UK during H2 2023', 'Hour', marker_size=3, line_style='markers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822a365",
   "metadata": {},
   "source": [
    "## Some observations from the data\n",
    "\n",
    "**Yearly average plot:**\n",
    "\n",
    "- Steady decrease over the years\n",
    "- Demand drops faster than generation, indicated by the growing gap between the too. UK has been selling more energy to adjacent energy markets (EU, Norway).\n",
    "\n",
    "**Monthly average plot:**\n",
    "\n",
    "- Yearly cycles.\n",
    "- Energy consumption increases during winter. \n",
    "\n",
    "**Weekly average plot:**\n",
    "\n",
    "- Demand drops significantly around the end of year.\n",
    "\n",
    "**Daily average plot:**\n",
    "\n",
    "- Weekly cycle.\n",
    "- Less demand on weekends.\n",
    "- Less demand on holidays (Christmas, Easter, New Year)\n",
    "\n",
    "**Hourly average plot:**\n",
    "\n",
    "- Daily cycle.\n",
    "- High demand on workdays between 07:00 and 20:00.\n",
    "- Also during the weekend, but to a lesser extend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3feaa8",
   "metadata": {},
   "source": [
    "## Forecasting Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cfac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define train and test period\n",
    "train_start = datetime(2023, 11, 1, 0, 0)\n",
    "train_end = datetime(2023, 11, 30, 23, 59)\n",
    "test_start = datetime(2023, 12, 1, 0, 0)\n",
    "test_end = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "# Get the full actual data for plotting our forecasts (from 1st Dec to 31st Dec)\n",
    "df_december = (\n",
    "    df.with_columns([\n",
    "        pl.col(\"datetime\"),\n",
    "        pl.col(\"tsd\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "actual_future_mask = (df_december[\"datetime\"] >= test_start) & (df_december[\"datetime\"] <= test_end)\n",
    "actual_future_times = df_december.filter(actual_future_mask)[\"datetime\"].to_pandas()\n",
    "actual_future_tsd = df_december.filter(actual_future_mask)[\"tsd\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86404921",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize results lists to store results RMSE and MAPE for each model\n",
    "results_7_day_mean = []\n",
    "results_ML = []\n",
    "results_ML_extra_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450c436",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Count how many 48h windows fit in the test period with 24h step to set colormap\n",
    "rolling_test_start = test_start\n",
    "window_count = 0\n",
    "while rolling_test_start + pd.Timedelta(hours=48) <= test_end:\n",
    "    window_count += 1\n",
    "    rolling_test_start += pd.Timedelta(hours=24)\n",
    "\n",
    "# Create colormap\n",
    "cmap = cm.get_cmap('berlin', window_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762cb26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to plot rolling 48h window predictions\n",
    "\n",
    "def plot_rolling_predictions(\n",
    "    times_list, preds_list, actual_times, actual_values, \n",
    "    colors, linestyles, title, xlabel, ylabel, \n",
    "    figsize=(18, 10), legend_loc='lower left', y_min=0\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=figsize)\n",
    "    # Plot actual\n",
    "    for t, v, ls in zip(actual_times, actual_values, linestyles):\n",
    "        plt.plot(t, v, color='black', linestyle=linestyles[0], label='Actual', zorder=1)\n",
    "    # Plot predictions\n",
    "    for i, (times, preds) in enumerate(zip(times_list, preds_list)):\n",
    "        if i == 0:\n",
    "            plt.plot(times, preds, color=colors(i), linestyle=linestyles[1], linewidth=4, alpha=0.9, label='Forecast', zorder=2)\n",
    "        else:\n",
    "            plt.plot(times, preds, color=colors(i), linestyle=linestyles[1], linewidth=4, alpha=0.9, zorder=2)\n",
    "    plt.legend(loc=legend_loc, fontsize=18)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.ylim(bottom=y_min)\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a87d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to perform rolling window predictions and evaluations\n",
    "\n",
    "def rolling_window_predictions(\n",
    "    df_ml, test_start, test_end, window_hours, step_hours, \n",
    "    predict_func, cmap, plot_actual=None, plot_title=\"\", \n",
    "    xlabel=\"Datetime\", ylabel=\"Demand (MW)\"\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    results = []\n",
    "    times_list = []\n",
    "    preds_list = []\n",
    "    rolling_test_start = test_start\n",
    "    i = 0\n",
    "    while rolling_test_start + pd.Timedelta(hours=step_hours) <= test_end:\n",
    "        test_mask = (df_ml[\"datetime\"] >= rolling_test_start) & (df_ml[\"datetime\"] < rolling_test_start + pd.Timedelta(hours=window_hours))\n",
    "        test_df = df_ml.filter(test_mask)\n",
    "        times = test_df[\"datetime\"].to_pandas()\n",
    "        y_test = test_df[\"tsd\"].to_numpy()\n",
    "        preds = predict_func(test_df, times)\n",
    "        rmse = root_mean_squared_error(y_test, preds)\n",
    "        mape = mean_absolute_percentage_error(y_test, preds)\n",
    "        results.append({\"test_start\": rolling_test_start, \"rmse\": rmse, \"mape\": mape})\n",
    "        times_list.append(times)\n",
    "        preds_list.append(preds)\n",
    "        if i == 0:\n",
    "            first_window = pd.DataFrame({\n",
    "                \"datetime\": times,\n",
    "                \"prediction\": preds\n",
    "            })\n",
    "        rolling_test_start += pd.Timedelta(hours=step_hours)\n",
    "        i += 1\n",
    "    # Plot\n",
    "    plot_rolling_predictions(\n",
    "        times_list, preds_list, \n",
    "        [plot_actual[0]] if plot_actual else [], [plot_actual[1]] if plot_actual else [],\n",
    "        cmap, ['solid', 'dotted'],\n",
    "        plot_title, xlabel, ylabel\n",
    "    )\n",
    "    return results, first_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4e0f4",
   "metadata": {},
   "source": [
    "**Baseline Model Predictions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0755122",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Baseline prediction function: 7-day mean demand at the same time of day\n",
    "\n",
    "def baseline_predict_func(test_df, times):\n",
    "    baseline_pred = []\n",
    "    for dt in times:\n",
    "        mask = (\n",
    "            (df_december[\"datetime\"] < dt) &\n",
    "            (df_december[\"datetime\"] >= dt - pd.Timedelta(days=7)) &\n",
    "            (df_december[\"datetime\"].dt.hour() == dt.hour) &\n",
    "            (df_december[\"datetime\"].dt.minute() == dt.minute)\n",
    "        )\n",
    "        vals = df_december.filter(mask)[\"tsd\"].to_numpy()\n",
    "        baseline_pred.append(np.nanmean(vals) if len(vals) > 0 else np.nan)\n",
    "    return np.array(baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e38416",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Vizualize baseline model predictions\n",
    "results_baseline, first_window_baseline = rolling_window_predictions(\n",
    "    df_december, test_start, test_end, 48, 24, baseline_predict_func, cmap,\n",
    "    plot_actual=(actual_future_times, actual_future_tsd),\n",
    "    plot_title=\"Actual Demand (TSD) and Baseline Predictions (48h windows)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7425df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print the scores\n",
    "results_baseline_pd = pd.DataFrame(results_baseline)\n",
    "print(f\"Mean Test RMSE: {results_baseline_pd['rmse'].mean():.2f} MW\")\n",
    "print(f\"Mean Test MAPE: {results_baseline_pd['mape'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4c981",
   "metadata": {},
   "source": [
    "**ML models' predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7f58e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Prepare features for ML \n",
    "df_ml = (\n",
    "    df.with_columns([\n",
    "        pl.col(\"datetime\"),\n",
    "        pl.col(\"tsd\"),\n",
    "        pl.col(\"datetime\").dt.ordinal_day().alias(\"day_of_year\"),  # 1-366\n",
    "        pl.col(\"datetime\").dt.weekday().alias(\"day_of_week\"),      # 0=Monday, 6=Sunday\n",
    "        pl.col(\"datetime\").dt.hour().alias(\"hour\"),                # 0-23\n",
    "        pl.col(\"tsd\").shift(1).alias(\"demand_lag_30m\"),            # Demand 30 minutes prior\n",
    "        pl.col(\"tsd\").shift(48).alias(\"demand_lag_24h\"),           # Demand 24 hours prior\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b8664",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize two instances of GradientBoostingRegressor model\n",
    "model_3 = GradientBoostingRegressor(\n",
    "        n_estimators=100, max_depth=5, learning_rate=0.1, validation_fraction=0.2, random_state=7\n",
    "    )\n",
    "model_5 = GradientBoostingRegressor(\n",
    "        n_estimators=100, max_depth=5, learning_rate=0.1, validation_fraction=0.2, random_state=7\n",
    "    )\n",
    "\n",
    "# Define features and targets\n",
    "features_3 = [\"day_of_week\", \"hour\", \"demand_lag_24h\"]\n",
    "features_5 = [\"day_of_year\", \"day_of_week\", \"hour\", \"demand_lag_30m\", \"demand_lag_24h\"]\n",
    "target = \"tsd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c47c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Prepare feature matrices and target vector\n",
    "X_3 = df_ml.select(features_3)\n",
    "X_5 = df_ml.select(features_5)\n",
    "y = df_ml.select(target)\n",
    "\n",
    "# Prepare training data\n",
    "train_mask = (df_ml[\"datetime\"] >= train_start) & (df_ml[\"datetime\"] < test_start)\n",
    "\n",
    "X_3_train = X_3.filter(train_mask).to_pandas()\n",
    "X_5_train = X_5.filter(train_mask).to_pandas()\n",
    "y_train = y.filter(train_mask).to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2655973",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train both models\n",
    "model_3.fit(X_3_train, y_train)\n",
    "model_5.fit(X_5_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee4efb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# For model_3 (3 features)\n",
    "def ml_predict_func_3(test_df, times):\n",
    "    X_test = test_df[features_3].to_pandas()\n",
    "    return model_3.predict(X_test)\n",
    "\n",
    "# For model_5 (5 features)\n",
    "def ml_predict_func_5(test_df, times):\n",
    "    X_test = test_df[features_5].to_pandas()\n",
    "    return model_5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bca43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run rolling predictions for each model\n",
    "results_ml_3, first_window_ml_3 = rolling_window_predictions(\n",
    "    df_ml, test_start, test_end, 48, 24, ml_predict_func_3, cmap,\n",
    "    plot_actual=(actual_future_times, actual_future_tsd),\n",
    "    plot_title=\"Actual Demand (TSD) and Model 3 Predictions (48h windows)\"\n",
    ")\n",
    "\n",
    "results_ml_5, first_window_ml_5 = rolling_window_predictions(\n",
    "    df_ml, test_start, test_end, 48, 24, ml_predict_func_5, cmap,\n",
    "    plot_actual=(actual_future_times, actual_future_tsd),\n",
    "    plot_title=\"Actual Demand (TSD) and Model 5 Predictions (48h windows)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064c1ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print the scores for model 3\n",
    "results_ml_3_pd = pd.DataFrame(results_ml_3)\n",
    "print(f\"Mean Test RMSE: {results_ml_3_pd['rmse'].mean():.2f} MW\")\n",
    "print(f\"Mean Test MAPE: {results_ml_3_pd['mape'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf39d99",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print the scores for model 5\n",
    "results_ml_5_pd = pd.DataFrame(results_ml_5)\n",
    "print(f\"Mean Test RMSE: {results_ml_5_pd['rmse'].mean():.2f} MW\")\n",
    "print(f\"Mean Test MAPE: {results_ml_5_pd['mape'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25b82c",
   "metadata": {},
   "source": [
    "## Forecast comparison: the first 48h window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5e469",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Prepare data for the first 48h window\n",
    "actual_times = first_window_baseline[\"datetime\"]\n",
    "actual_values = df_ml.filter(\n",
    "    (df_ml[\"datetime\"] >= actual_times.iloc[0]) & (df_ml[\"datetime\"] <= actual_times.iloc[-1])\n",
    ")[\"tsd\"].to_numpy()\n",
    "\n",
    "# Forecasts for the first 48h window\n",
    "baseline_pred = first_window_baseline[\"prediction\"]\n",
    "gbr_pred = first_window_ml_3[\"prediction\"]\n",
    "gbr_extra_pred = first_window_ml_5[\"prediction\"]\n",
    "\n",
    "# Plotly colors (same as in the last block)\n",
    "color_baseline = '#ff7f0e'\n",
    "color_gbr = '#bcbd22'\n",
    "color_gbr_extra = '#2ca02c'\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Actual (solid black)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=actual_times,\n",
    "    y=actual_values,\n",
    "    mode='lines',\n",
    "    name='Actual',\n",
    "    line=dict(color='black', width=2, dash='solid')\n",
    "))\n",
    "\n",
    "# Baseline forecast (dotted)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=actual_times,\n",
    "    y=baseline_pred,\n",
    "    mode='lines+markers',\n",
    "    name='Baseline: 7-day mean',\n",
    "    line=dict(color=color_baseline, width=3, dash='dot'),\n",
    "    marker=dict(color=color_baseline)\n",
    "))\n",
    "\n",
    "# GBR 3 features (dotted)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=actual_times,\n",
    "    y=gbr_pred,\n",
    "    mode='lines+markers',\n",
    "    name='GBR (3 features)',\n",
    "    line=dict(color=color_gbr, width=3, dash='dot'),\n",
    "    marker=dict(color=color_gbr)\n",
    "))\n",
    "\n",
    "# GBR 5 features (dotted)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=actual_times,\n",
    "    y=gbr_extra_pred,\n",
    "    mode='lines+markers',\n",
    "    name='GBR (5 features)',\n",
    "    line=dict(color=color_gbr_extra, width=3, dash='dot'),\n",
    "    marker=dict(color=color_gbr_extra)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"First 48h Window: Actual vs Forecasts\",\n",
    "    xaxis_title=\"Datetime\",\n",
    "    yaxis_title=\"Demand (MW)\",\n",
    "    title_font=dict(size=20),\n",
    "    title_x=0.5,\n",
    "    plot_bgcolor='whitesmoke',\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    width=1100,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81552b5",
   "metadata": {},
   "source": [
    "## Forecast comparison: RMSE throught the whole month of December 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204405c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Make x axis go from 1 to 31 (all December days)\n",
    "x_days = list(range(1, 32))\n",
    "\n",
    "# Prepare y values for each model, filling missing days with None for gaps\n",
    "def fill_rmse_by_day(results_df):\n",
    "    rmse_by_day = {d.day: v for d, v in zip(results_df[\"test_start\"], results_df[\"rmse\"])}\n",
    "    return [rmse_by_day.get(day, None) for day in x_days]\n",
    "\n",
    "y_7d = fill_rmse_by_day(pd.DataFrame(results_baseline))\n",
    "y_ml_3 = fill_rmse_by_day(pd.DataFrame(results_ml_3))\n",
    "y_ml_5 = fill_rmse_by_day(pd.DataFrame(results_ml_5))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_days,\n",
    "    y=y_7d,\n",
    "    mode='lines+markers',\n",
    "    name='Baseline: 7-day mean',\n",
    "    line=dict(color='#ff7f0e')\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_days,\n",
    "    y=y_ml_3,\n",
    "    mode='lines+markers',\n",
    "    name='GBR',\n",
    "    line=dict(color='#bcbd22')\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_days,\n",
    "    y=y_ml_5,\n",
    "    mode='lines+markers',\n",
    "    name='GBR with extra features',\n",
    "    line=dict(color='#2ca02c')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparison of different energy demand forecasting methods for December 2023 in the UK\",\n",
    "    xaxis_title=\"Day of December\",\n",
    "    yaxis_title=\"RMSE (MW)\",\n",
    "    plot_bgcolor='whitesmoke',\n",
    "    legend=dict(\n",
    "        font=dict(size=18),\n",
    "        x=1,\n",
    "        y=1,\n",
    "        xanchor='right',\n",
    "        yanchor='top'\n",
    "    ),\n",
    "    title_font=dict(size=20),\n",
    "    title_x=0.5,\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=1,\n",
    "        dtick=1,\n",
    "        range=[0.1, 31],\n",
    "        tickfont=dict(size=16),\n",
    "        title_font=dict(size=18)\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[0, 6999],\n",
    "        tickfont=dict(size=16), \n",
    "        title_font=dict(size=18)\n",
    "        ),\n",
    "    width=1100,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144d643",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e8d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test functions for feature importance plot and values\n",
    "def test_feature_importance_plot(model, features):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = features\n",
    "\n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    sorted_names = [feature_names[i] for i in indices]\n",
    "    sorted_importances = importances[indices]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(sorted_names, sorted_importances, color='dodgerblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances for GBR (5 features)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Verify normalized importance values\n",
    "def test_feature_importance_values(model, features):\n",
    "    importances = model.feature_importances_\n",
    "    assert len(importances) == len(features)\n",
    "    assert np.isclose(importances.sum(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7a5a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Feature importance test for model 3\n",
    "test_feature_importance_plot(model_3, features_3)\n",
    "test_feature_importance_values(model_3, features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e14c22",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Feature importance test for model 5\n",
    "test_feature_importance_plot(model_5, features_5)\n",
    "test_feature_importance_values(model_5, features_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523c56c",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "**Lagged demand features are the most important**: Including demand_lag_30m (previous 30 min) and to a lesser extend demand_lag_24h (previous day) as features improves the model's ability to forecast demand, as shown by the lower RMSE and MAPE for the GBR models compared to the baseline.\n",
    "\n",
    "**Temporal features help:** Features like day_of_week, hour, and day_of_year capture seasonality and daily/weekly cycles, which are important for electricity demand forecasting.\n",
    "\n",
    "**Model performance improves with more features:** The GBR model with 5 features outperforms the one with only 3, indicating that additional temporal and lagged features add predictive value. \n",
    "\n",
    "**Baseline is outperformed:** Both GBR models provide better accuracy than the simple 7-day mean baseline, demonstrating the value of machine learning and feature engineering for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aba67b",
   "metadata": {},
   "source": [
    "## Ways to improve\n",
    "\n",
    "**Add weather data:** temperature could be useful to predict heating/cooling associated electricity demands.\n",
    "\n",
    "**Add more lagged features:** 1h, 3h, 12h, 1 week (and explore which ones are most important).\n",
    "\n",
    "**Tune hyperparameters:** Use grid search or Bayesian optimization to find the best model parameters.\n",
    "\n",
    "**Handle special cases:** Daylight saving creates duplicate/missing time intances, which could affect forecasting.\n",
    "\n",
    "**Ensemble methods:** Combine predictions from multiple models to reduce variance and improve accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
